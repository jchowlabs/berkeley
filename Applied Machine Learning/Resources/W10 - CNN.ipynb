{"cells":[{"cell_type":"markdown","metadata":{"id":"Fd0bWRjBDziA"},"source":["# Week 10 - CNN"]},{"cell_type":"markdown","metadata":{"id":"L_Tt0zGuDziB"},"source":["#### ``Objectives``\n","1. Implement a CNN to detect diabetic retinopathy (DR) from retina images taken using fundus photography under a variety of imaging conditions\n","2. Improve generalization performance and reduce overfitting using **image transformation** and **data augmentation** techniques\n","3. Tune hyperparameters of the CNN model"]},{"cell_type":"markdown","metadata":{"id":"Ft110s8xDziC"},"source":["#### ``Motivation``"]},{"cell_type":"markdown","metadata":{"id":"-qvclPUEDziC"},"source":["Diabetic retinopathy (DR) is an eye condition that  affects blood vessels in the retina. It can cause vision loss and blindness in people who have diabetes. Screening for DR allows earlier and more effective treatment options for millions of people."]},{"cell_type":"markdown","metadata":{"id":"Di34_ZXPDziC"},"source":["#### ``Data``"]},{"cell_type":"markdown","metadata":{"id":"V8nYiGqWDziC"},"source":["In this assignment you will use a small dataset of retina images (`Download` links: [images](https://drive.google.com/drive/folders/1sdfUC64Un1iwuiHEehcbijxB54OhU_nd?usp=sharing) and [labels](https://drive.google.com/drive/folders/1MOlSJBZg7L1HtG5vHPt77ighRvQaGfDg?usp=sharing)). You will **build** and **train** a **CNN model** to predict whether or not to refer a patient for DR treatment using binarized severity of DR in patients: no referral if {No DR, mild} and referral if {moderate, severe, and proliferate DR}.\n","\n","\n","<u>Note</u>: the original dataset is hosted by Kaggle [[Source]](https://www.kaggle.com/c/aptos2019-blindness-detection/)."]},{"cell_type":"markdown","metadata":{"id":"fMn50w1-DziC"},"source":["Import the necessary libraries and make sure to replace IMAGE_PATH and LABEL_PATH with the path to the directories where you saved the data."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"G-s7gOUwDziD","executionInfo":{"status":"ok","timestamp":1657556432132,"user_tz":420,"elapsed":5386,"user":{"displayName":"Clarence Chio Wen Han","userId":"05962720028099161137"}}},"outputs":[],"source":["# standard\n","import pandas as pd\n","import numpy as np\n","import random\n","import os\n","\n","# tf and keras\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n","from keras import models\n","from keras import layers\n","\n","# plots\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","random.seed(2)\n","%matplotlib inline\n","\n","# FILL IN CODE HERE #\n","IMAGE_PATH = '' # replace with your path\n","LABEL_PATH = '' # replace with your path"]},{"cell_type":"markdown","metadata":{"id":"vaeTEAstDziE"},"source":["At this point, you may ask yourself what the best CNN model that fits this data is. First, you will want to read through the data description in Kaggle (see the link to the original dataset above). Understanding what you are working with challenges you to write preprocessing code that uncovers your data's most helpful information."]},{"cell_type":"markdown","metadata":{"id":"jXYIBQIwDziE"},"source":["---\n","### Exercise 1 (8 points)"]},{"cell_type":"markdown","metadata":{"id":"dgkq4g64DziE"},"source":["Read the data description from Kaggle and list (a) the source of images and (b) the labeling procedure."]},{"cell_type":"markdown","metadata":{"id":"H-aPx9QkDziE"},"source":["*Written answer*:"]},{"cell_type":"markdown","metadata":{"id":"G0O8x-9iDziE"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"xQzkZD5WDziF"},"source":["## Read data"]},{"cell_type":"markdown","metadata":{"id":"VpRTdBWnDziF"},"source":["Let's now explore our dataset. We will start with label inspection and continue with image visualization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blNL6mQBDziF"},"outputs":[],"source":["y = pd.read_csv(\n","    LABEL_PATH + 'labels.csv'\n",")\n","\n","print('Shape of labels:', y.shape)\n","print('Unique diagnosis codes:', np.sort(y.diagnosis.unique()))\n","y.head()"]},{"cell_type":"markdown","metadata":{"id":"4GJHyJOaDziF"},"source":["There are 200 training images corrresponding to 5 different diabetic retinopathy (DR) diagnosis codes: \n","\n","* No DR (0)\n","* mild (1)\n","* moderate (2)\n","* severe (3)\n","* proliferate DR (4)\n"]},{"cell_type":"markdown","metadata":{"id":"PIDOks_BDziF"},"source":["Image inspection:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcjXGupoDziF"},"outputs":[],"source":["# read image\n","indx=0\n","images = []\n","\n","print('Sample of images in data:')\n","for idx, img in enumerate(os.listdir(IMAGE_PATH)):\n","    img = load_img(\n","    IMAGE_PATH + img)\n","    images.append(img)\n","\n","nrows, ncols = 2,4 #print first 8 images\n","f, axs = plt.subplots(nrows, ncols, figsize=(20,10))\n","for i in range(nrows):\n","    for j in range(ncols):\n","        indx = i*nrows+j\n","        axs[i,j].imshow(images[indx])"]},{"cell_type":"markdown","metadata":{"id":"PpzX04pIDziF"},"source":["Like any real-world data set, you can see that these images have different sizes and focus. Later, you will use image transformation and data augmentation techniques to remove some of this variation."]},{"cell_type":"markdown","metadata":{"id":"P7dVzgM-DziF"},"source":["## Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"aEPErW8LDziG"},"source":["The quality of the data determines how well a machine learning algorithm can learn. This section will apply some simple techniques to deal with class imbalance. We will then create training/validation/test datasets and perform image transformation and augmentation."]},{"cell_type":"markdown","metadata":{"id":"tsGwB9yQDziG"},"source":["---\n","### Exercise 2 (8 points)"]},{"cell_type":"markdown","metadata":{"id":"au52nTTdDziG"},"source":["Graph a histogram for the five classes of DR. Write down the percentage of the largest class in the written answer below. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IS_zk2aDDziG"},"outputs":[],"source":["# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"INjWxePTDziG"},"source":["*Written answer*:"]},{"cell_type":"markdown","metadata":{"id":"nYL8ZmF8DziG"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"i12GwfNyDziG"},"source":["`Correct for data imbalance`"]},{"cell_type":"markdown","metadata":{"id":"JQdTpmFXDziG"},"source":["As you can see from the histogram above, our dataset is very imbalanced, which is common in healthcare, and it happens because some diseases are rare. The presence of imbalanced data hampers the detection of rare events as most classification methods implicitly assume a similar occurrence of classes and are designed to maximize the overall classification accuracy.\n","\n","We will correct for class imbalance in two ways:\n","\n","  * First, we will binarize the DR diagnosis as follows:\n","     - 'no refer' are {No DR, mild}\n","     - 'refer' are {Moderate, Severe, Proliferate}\n","\n","\n","  * Second, we'll only take 80 random samples from the 'no refer' class and 80 from the 'refer' class.\n","\n","It is a crude method to deal with imbalanced data, but it will be good enough for this homework. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8E9QPLA7DziG"},"outputs":[],"source":["np.random.seed(1234)\n","\n","no_refer = y[y.diagnosis.isin((0,1))]\n","refer = y[y.diagnosis.isin((2,3,4))]\n","\n","# randomly draw 80 images from each classes\n","temp_no_refer = list(np.random.choice(\n","    no_refer.id_code,\n","    size=80,\n","    replace=False\n","))\n","\n","temp_refer = list(np.random.choice(\n","    refer.id_code,\n","    size=80,\n","    replace=False\n","))"]},{"cell_type":"markdown","metadata":{"id":"zubuwJnbDziG"},"source":["Next, we will use the **preprocess_data_part1()** function defined below to generate lists of images and labels (`images_mini` and `y_mini`) based on the values in the temp_no_refer and temp_refer lists."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5qjkhulDziG"},"outputs":[],"source":["def preprocess_data_part1(IMAGE_PATH, LABEL_PATH):\n","    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n","    \n","    Params:\n","    -------\n","    IMAGE_PATH (str): path to directory with images.\n","    LABEL_PATH (str): path to directory with labels.\n","    \n","    Returns:\n","    --------\n","    images_mini  (np.ndarray): Images of shape (N, 224, 224, 3)\n","    y_mini (np.ndarray): Labels of shape (N,)    \n","    \"\"\"\n","    y_mini = []\n","    images_mini = []\n","\n","    # create lists of images and labels `images_mini` and `y_mini` \n","    # based on temp_no_refer and temp_refer selections\n","    for idx, img in enumerate(os.listdir(IMAGE_PATH)):\n","        # read labels\n","        if img.split('.')[0] in temp_no_refer:\n","                y_mini.append(0)\n","        elif img.split('.')[0] in temp_refer:\n","                y_mini.append(1)\n","        else:\n","            continue\n","\n","        \n","        # read image\n","        img = load_img(\n","            IMAGE_PATH + img,\n","            target_size=(224, 224)\n","        )\n","        \n","        # transform image to array\n","        img = img_to_array(img)\n","\n","        # append to images\n","        images_mini.append(img)\n","\n","    # stack images and trasnform to array\n","    images_mini = np.stack(images_mini)\n","    y_mini = np.array(y_mini).flatten() \n","    \n","    return images_mini, y_mini"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx574bXrDziH"},"outputs":[],"source":["# generate images and labels based on preprocess_data_part1() function\n","images_mini, y_mini = preprocess_data_part1(IMAGE_PATH, LABEL_PATH)\n","\n","print(f\"images_mini shape {images_mini.shape}\")\n","print(f\"y_mini shape {y_mini.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"LoCjynVnDziH"},"source":["`Create train/validation/test data` and ``perform image tranformation and augmentation``"]},{"cell_type":"markdown","metadata":{"id":"JSOMUvtXDziH"},"source":["The next step in the data preprocessing part is to split the data into training, validation, and test sets. Once we have these partitions, we will apply image transformation and augmentations.\n","\n","\n","To give you an idea of what image transformation and augmentation do, let's see an example applied to the first image in our mini data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNlA5RKRDziH"},"outputs":[],"source":["tf.random.set_seed(1234)\n","\n","fig = plt.figure(figsize=(14, 12))\n","\n","# pull first image from data\n","image = images_mini[0]\n","\n","# plot original\n","ax = fig.add_subplot(1, 5, 1)\n","ax.imshow(array_to_img(image))\n","ax.set_title('Original', size=15);\n","\n","# resize\n","ax = fig.add_subplot(1, 5, 2)\n","img_resize = tf.image.resize(image, size=(224, 224))\n","ax.imshow(array_to_img(img_resize))\n","ax.set_title('Step 1: Resize', size=15);\n","\n","\n","# adjust brightness\n","ax = fig.add_subplot(1, 5, 3)\n","img_bright = tf.image.adjust_brightness(img_resize, 0.3)\n","ax.imshow(array_to_img(img_bright))\n","ax.set_title('Step 2: Brightness', size=15);\n","\n","\n","# adjust contrast\n","ax = fig.add_subplot(1, 5, 4)\n","img_contrast = tf.image.adjust_contrast(img_bright, contrast_factor=3)\n","ax.imshow(array_to_img(img_contrast))\n","ax.set_title('Step 3: Contrast', size=15);\n","\n","\n","# flip left right\n","ax = fig.add_subplot(1, 5, 5)\n","img_flip = tf.image.flip_left_right(img_contrast)\n","ax.imshow(array_to_img(img_flip))\n","ax.set_title('Step 4: Flip left right', size=15);"]},{"cell_type":"markdown","metadata":{"id":"AqNf5i_3DziH"},"source":["Next, we will define and run the **preprocess_data_part2()** function to create:\n","\n","* train/validation/test sets with split (0.6,0.2,0.2)\n","\n","* image transformation and augmentation, as follows:\n","\n","<u>Applied on training, validation and test sets</u>:\n","  - resize to IMAGE_SIZE =(224,224) using tf.image.resize()\n","  - normalize all pixel values to the range (0,1)\n","  \n","<u>Applied on training set only</u> (note that this step will create additional/augmented copies of the training data):\n","  - adjust brightness by adding DELTA=0.3 to the pixel values using tf.image.adjust_brighness()\n","  - adjust contrast to CONTRAST_FACTOR=3 using tf.image.adjust_contrast()\n","  - flip left right using tf.image.flip_left_right()"]},{"cell_type":"markdown","metadata":{"id":"xdO86plKDziH"},"source":["The quantity and diversity of data gathered have a significant impact on the results of a CNN model. One can apply augmentations to artificially inflate the training dataset by warping the original data such that their label does not change. These augmentations can significantly improve learning results without collecting new data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_9NLrFADziH"},"outputs":[],"source":["IMAGE_SIZE = (224, 224)\n","CONTRAST_FACTOR = 3\n","DELTA = 0.3\n","\n","def preprocess_data_part2(images, y, split=(0.6,0.2,0.2)):\n","    \"\"\" Split data into train, validation and test sets; apply transformaions and augmentations\n","    \n","    Params:\n","    -------\n","    images  (np.ndarray): Images of shape (N, 224, 224, 3)\n","    y (np.ndarray): Labels of shape (N,)   \n","    split (tuple): 3 values summing to 1 defining split of train, validation and test sets\n","    \n","    Returns:\n","    --------\n","    X_train (np.ndarray): Train images of shape (N_train, 224, 224, 3)\n","    y_train (np.ndarray): Train labels of shape (N_train,)\n","    X_val (np.ndarray): Val images of shape (N_val, 224, 224, 3)\n","    y_val (np.ndarray): Val labels of shape (N_val,)\n","    X_test (np.ndarray): Test images of shape (N_test, 224, 224, 3)\n","    y_test (np.ndarray): Test labels of shape (N_test,)\n","    \n","    \"\"\"\n","    \n","    ### create train/validation/test sets ###\n","    #########################################\n","    # NOTE: Each time you run this cell, you'll re-shuffle the data. The ordering will be the same due to the random seed generator \n","    tf.random.set_seed(1234)\n","    np.random.seed(1234)\n","    shuffle = np.random.permutation(np.arange(images.shape[0]))\n","    images, y = images[shuffle], y[shuffle]\n","    \n","    splits = np.multiply(len(images_mini), split).astype(int)\n","    X_train, X_val, X_test = np.split(images_mini, [splits[0], splits[0]+splits[1]])\n","    y_train, y_val, y_test = np.split(y_mini, [splits[0], splits[0]+splits[1]])\n","    \n","    ### image transformation on training, validation, and test data ###\n","    ###################################################################\n","    # image resize\n","    X_train = tf.image.resize(X_train, size=IMAGE_SIZE)\n","    X_val = tf.image.resize(X_val, size=IMAGE_SIZE)\n","    X_test = tf.image.resize(X_test, size=IMAGE_SIZE)\n","    \n","    # rescale image to [0,1], i.e., greyscale\n","    X_train = X_train/255.0\n","    X_val = X_val/255.0\n","    X_test = X_test/255.0\n","    \n","    \n","    ### image augmentation on training data ###\n","    ###########################################\n","    # adjust brightness\n","    X_train_augm = tf.image.adjust_brightness(X_train, delta=DELTA)\n","    \n","    # adjust contrast\n","    X_train_augm = tf.image.adjust_contrast(X_train_augm, contrast_factor=CONTRAST_FACTOR)\n","\n","    # random flip\n","    X_train_augm = tf.image.random_flip_left_right(X_train_augm)\n","    \n","    # concatenate original X_train and augmented X_train data\n","    X_train = tf.concat([X_train, X_train_augm],axis=0)\n","    \n","    # concatenate y_train (note the label is preserved)\n","    y_train_augm = y_train\n","    y_train = tf.concat([y_train, y_train_augm],axis=0)\n","    \n","    # shuffle X_train and y_train, i.e., shuffle two tensors in the same order\n","    shuffle = tf.random.shuffle(tf.range(tf.shape(X_train)[0], dtype=tf.int32))\n","    X_train = tf.gather(X_train, shuffle)\n","    y_train = tf.gather(y_train, shuffle).numpy() #also transforms y_train to numpy array\n","    \n","    return X_train, y_train, X_val, y_val, X_test, y_test"]},{"cell_type":"markdown","metadata":{"id":"W25w0cnFDziH"},"source":["Let's sanity check our implementation of the preprocess_data_part2() function by printing the shape of (X,y) from train, validation and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpDGqR8JDziH"},"outputs":[],"source":["X_train, y_train, X_val, y_val, X_test, y_test = preprocess_data_part2(\n","    images_mini,\n","    y_mini,\n","    split=(0.6,0.2,0.2)\n",")\n","\n","print(f\"X_train shape {X_train.shape}\")\n","print(f\"y_train shape {y_train.shape}\")\n","print(f\"X_val shape {X_val.shape}\")\n","print(f\"y_val shape {y_val.shape}\")\n","print(f\"X_test shape {X_test.shape}\")\n","print(f\"y_test shape {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"jlT0UZxpDziI"},"source":["Let's also print out the first 8 train and validation examples with the label of each example as the title."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7XosdgdDziI"},"outputs":[],"source":["# print taining data\n","print('Print training data examples:')\n","nrows, ncols = 1,4 #print first 4 images\n","f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n","for i in range(ncols):\n","    axs[i].imshow(array_to_img(X_train[i]))\n","    axs[i].set(title=y_train[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VmzK6tWDziI"},"outputs":[],"source":["# print test data\n","print('Print validation data examples:')\n","nrows, ncols = 1,4 #print first 4 images\n","f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n","for i in range(ncols):\n","    axs[i].imshow(array_to_img(X_val[i]))\n","    axs[i].set(title=y_val[i])"]},{"cell_type":"markdown","metadata":{"id":"jmfUhECBDziI"},"source":["---\n","### Exercise 3 (8 points)\n","\n","We've split the data into train, validation, and test sets. Explain the purpose of each of these.\n"]},{"cell_type":"markdown","metadata":{"id":"tO7Uq05WDziI"},"source":["*Written answer*:"]},{"cell_type":"markdown","metadata":{"id":"4bx4Xz3eDziI"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"ijhCR_3pDziI"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"v5Fie7awDziI"},"source":["Our objective is to build and train a CNN model to refer patients to doctors based on the severity of DR seen in these images. We are interested in exploring binary classification of 'no refer' and 'refer'.\n","\n","<u>The architecture of our CNN model is as follows</u>:\n","\n","1. the model receives input images of size 224 x 224 x 3 (the images have three color channels)\n","2. the input data goes through two convolutional layers that have kernels of size 5 x 5\n","3. the first convolution has 32 output feature maps, and the second one has 64\n","4. each convolution layer is followed by a max-pooling layer (this will reduce the size of the feature maps)\n","5. the last two layers of the model are fully connected with a droput layer in between\n","\n","For each convolution we use strides=(1,1) to preserve the dimension of the inputs in the resulting feature maps. For the pooling layers, we set strides=(2,2) to subsample the image and shrink the size of the output feature maps. For the dropout layer, we set the probability of dropping input units during training to 0.5.\n","\n","\n","We will implement this architecture using TensorFlow Keras API."]},{"cell_type":"markdown","metadata":{"id":"pFbJ96VoDziI"},"source":["``Build model``"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0Xbw2PkDziI"},"outputs":[],"source":["model = tf.keras.Sequential()\n","\n","# add first convolution layer to the model\n","model.add(tf.keras.layers.Conv2D(\n","    filters=32,\n","    kernel_size=(5, 5),\n","    strides=(1, 1),\n","    padding='same',\n","    data_format='channels_last',\n","    name='conv_1',\n","    activation='relu'))\n","\n","\n","# add a max pooling layer with pool size (2,2) and strides of 2\n","# (this will reduce the spatial dimensions by half)\n","model.add(tf.keras.layers.MaxPool2D(\n","    pool_size=(2, 2),\n","    name='pool_1'))\n","\n","\n","# add second convolutional layer\n","model.add(tf.keras.layers.Conv2D(\n","    filters=64,\n","    kernel_size=(5, 5),\n","    strides=(1, 1),\n","    padding='same',\n","    name='conv_2',\n","    activation='relu'))\n","\n","# add second max pooling layer with pool size (2,2) and strides of 2\n","# (this will further reduce the spatial dimensions by half)\n","model.add(tf.keras.layers.MaxPool2D(\n","    pool_size=(2, 2), name='pool_2')\n",")\n","\n","\n","# add a fully connected layer (need to flatten the output of the previous layers first)\n","model.add(tf.keras.layers.Flatten()) \n","model.add(tf.keras.layers.Dense(\n","    units=1024,\n","    name='fc_1', \n","    activation='relu'))\n","\n","# add dropout layer\n","model.add(tf.keras.layers.Dropout(\n","    rate=0.5))\n","\n","# add the last fully connected layer\n","# this last layer sets the activation function to \"None\" in order to output the logits \n","# note that passing activation = \"sigmoid\" will return class memembership probabilities but\n","# in TensorFlow logits are prefered for numerical stability\n","# set units=1 to get a single output unit (remember it's a binary classification problem)\n","model.add(tf.keras.layers.Dense(\n","    units=1,\n","    name='fc_2',\n","    activation=None))\n","\n","\n","# build model and print summary\n","tf.random.set_seed(1)\n","model.build(input_shape=(None, 224, 224, 3))\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"cbIEX1dlDziI"},"source":["``Compile model``"]},{"cell_type":"markdown","metadata":{"id":"ofN49gXaDziI"},"source":["The next step is to compile the model. We have to decide on the type of optimizer, loss function, and metrics to compute.\n","\n","We will use the Adam optimizer for this assignment, the most popular gradient-based optimization algorithm. There are a few other choices, and you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). The loss (cost) function suitable for our binary classification model is [binary_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses). We will compute model accuracy on the training, validation and test datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2NBOPEtDziI"},"outputs":[],"source":["model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), #set from_ligits=True because our last layer does not apply sigmoid\n","              metrics=['accuracy']) "]},{"cell_type":"markdown","metadata":{"id":"dVJaaAfTDziI"},"source":["``Fit model``"]},{"cell_type":"markdown","metadata":{"id":"yV2Z6MbsDziI"},"source":["Finally, we will fit the model with 10 epochs on the train set and validate on the validation set. The performance depends on the current starter hyperparameters such as learning rate and choice of optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRDCKffXDziI"},"outputs":[],"source":["# set random seed to get reproductible results \n","# neural network algorithms are stochastic (e.g., due to random weight initialization); setting a random seed helps to get more stable results after each run\n","# however, best way to deal with randomness is to repeat your experiment many times (30+) and use statistics to summarize the performance of the model\n","tf.random.set_seed(1234)\n","np.random.seed(1234)\n","history = model.fit(X_train, y_train,\n","                    epochs=10, \n","                    validation_data=(X_val, y_val)\n",")"]},{"cell_type":"markdown","metadata":{"id":"U8jwxFcxDziI"},"source":["Next let's plot loss and accuracy for training and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dj0H5m7DziJ"},"outputs":[],"source":["hist = history.history\n","x_arr = np.arange(len(hist['loss'])) + 1\n","\n","fig = plt.figure(figsize=(12, 4))\n","ax = fig.add_subplot(1, 2, 1)\n","ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n","ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n","ax.legend(fontsize=15)\n","ax.set_xlabel('Epoch', size=15)\n","ax.set_ylabel('Loss', size=15)\n","\n","ax = fig.add_subplot(1, 2, 2)\n","ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n","ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')\n","ax.legend(fontsize=15)\n","ax.set_xlabel('Epoch', size=15)\n","ax.set_ylabel('Accuracy', size=15)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rECjn_cWDziJ"},"source":["## Show what the CNN model is learning after each layer"]},{"cell_type":"markdown","metadata":{"id":"MQP4WvlrDziJ"},"source":["We will pick one example from our training data to visualize our CNN model's learning after each layer. Below we print the original image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88jBhDhnDziJ"},"outputs":[],"source":["img_tensor = np.expand_dims(X_train[2], axis = 0);\n","\n","# Print image tensor shape\n","print('Shape of image:', img_tensor.shape);\n","  \n","# Print image\n","plt.imshow(img_tensor[0]);\n","plt.title('label:' + str(y_train[1]))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3H2IGke8DziJ"},"source":["Next we print what the model learns after each layer-channel. It's important to pay attention to the shape of the output image to understand what each layer does to the original input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rx8OqdRfDziJ"},"outputs":[],"source":["# outputs of the first 4 layers, which include conv2D and max pooling layers\n","layer_outputs = [layer.output for layer in model.layers[:4]]\n","activation_model = models.Model(inputs = model.input, outputs = layer_outputs)\n","activations = activation_model.predict(img_tensor)\n","\n","# grab layer names\n","layer_names = []\n","for layer in model.layers[:4]:\n","    layer_names.append(layer.name)\n","\n","# getting activations of each layer\n","for idx, layer in enumerate(activations):\n","    if idx in (0,1,2,3):\n","        print('----------------')\n","        print('Geeting activations of layer',  idx+1, ':', layer_names[idx])\n","        activation = layer\n","\n","        # shape of layer activation\n","        print('Images size is', activation.shape[1], 'x', activation.shape[2])\n","        print('Number of channels is', activation.shape[3])\n","\n","        # print channels\n","        print('Printing channels:')\n","        \n","        # define nrows and ncols depending on number of channels\n","        if idx in (0,1):\n","            nrows, ncols = 4,8\n","        if idx in (2,3):\n","            nrows, ncols = 8,8\n","\n","        # plots\n","        channel=0\n","        if idx in (0,1):\n","            f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n","        if idx in (2,3):\n","            f, axs = plt.subplots(nrows, ncols, figsize=(14,20))\n","            \n","        for i in range(nrows):\n","            for j in range(ncols):\n","                if i==0 and j==0:\n","                    channel=0\n","                else:\n","                    channel+=1\n","\n","                axs[i,j].matshow(activation[0,:, :, channel], cmap ='viridis')\n","                axs[i,j].set(title=str(channel))\n","                #axs[i,j].axis('off') # pay attention to the range of x and y axis\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sXy6jLdnDziJ"},"source":["\n","As you can see, initial layers are more interpretable and retain the majority of the features in the input image. As the level of the layer increases, features become less interpretable, they become more abstract and they identify features specific to the class leaving behind the general features of the image."]},{"cell_type":"markdown","metadata":{"id":"xW_7mwTADziJ"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"yW8rbcWHDziJ"},"source":["Evaluation is one of the most important parts of machine learning as it helps us determine how good our trained model is in predicting unseen data.\n","\n","Notice that (`X_test`, and `y_test`) were not used in the training part. It would be very bad practice to evaluate the model on the test set, and then return and update the model based on those results (then the test set is acting like just another validation set). "]},{"cell_type":"markdown","metadata":{"id":"Ns7TUmI8DziJ"},"source":["We will now use our test data to evaluate the performance (accuracy) of our CNN model on unseen data. Note that accuracy is the default metric if one compiles the model with the accuracy metric."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-dyi0lpDziK"},"outputs":[],"source":["test_results = model.evaluate(X_test, y_test)\n","print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))"]},{"cell_type":"markdown","metadata":{"id":"oRFeZTmuDziK"},"source":["`get predictions results in the form of class-membership probabilities`"]},{"cell_type":"markdown","metadata":{"id":"qVmNtOU9DziK"},"source":["In the following figure, you can see all the images in the test data along with their ground truth (GT) labels and the predicted probabiliy that they belong to class 1, 'Refer' "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7cr1UOADziK"},"outputs":[],"source":["# transform logits to probabilities\n","pred_logits = model.predict(X_test)\n","probas = tf.sigmoid(pred_logits)\n","probas = probas.numpy().flatten()*100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgDd0yzzDziK"},"outputs":[],"source":["# plot test data and associated predicred\n","fig = plt.figure(figsize=(20, 20))\n","\n","for j, example in enumerate(X_test):\n","    ax = fig.add_subplot(8,4, j+1)\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    ax.imshow(array_to_img(example))\n","    if y_test[j]==0:\n","        label='no refer'\n","    else:\n","        label='refer'\n","    \n","    ax.text(\n","        0.5, -0.15, \n","        'GT: {:s}\\nPr(refer)={:.0f}%'.format(label, probas[j]), \n","        size=16, \n","        color='grey',\n","        horizontalalignment='center',\n","        verticalalignment='center', \n","        transform=ax.transAxes)\n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BCdFsk9PDziK"},"source":["---\n","### Exercise 4 (8 points)"]},{"cell_type":"markdown","metadata":{"id":"bXxrmUpSDziK"},"source":["In this exercise, your task is to experiment with different hyperparameter values to see how sensitive the accuracy of our CNN model is.  In the table below, you will fill in `training and validation accuracy` results corresponding to the last epoch of the model fit.\n","\n","You will change one hyperparameter value at a time, as follows: \n","\n","* kernel size = 3 x 3\n","* strides = 2 x 2\n","* pool size = 3 x 3\n","* learning rate = 0.01\n","* optimizer = 'SGD'\n","* image augmentation applied on the training data: (brightness (delta) = 0.1, contrast = 2, flip = no)\n","\n","Finally, comment on the CNN model specification that you are most satisfied with based on the accuracy achieved on the training and validation sets. For this specification, what is the accuracy of the test dataset?"]},{"cell_type":"markdown","metadata":{"id":"ETrqIaMxDziK"},"source":["*Written answer*:\n","\n","| Trining accuracy | Validation accuracy |  kernel size |  strides | pool size  |  learning rate | optimizer  | brightness (delta) |  contrast factor | flip_on_train  |\n","|:-:                |:-:                  |:-:           |:-:       |:-:         |:-:             |:-:         |:-:                 |:-:               |:-:             |\n","| 0.94              | 0.81                | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | 3                | yes            |\n","| ...               | ...                 | <font color=\"red\">3,3</font>     | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | 3                | yes            |\n","| ...               | ...                 | 5,5          | <font color=\"red\">2,2</font>  | 2,2        | 0.001          | Adam       | 0.3                | 3                | yes            |\n","| ...               | ...                 | 5,5          | 1,1      | <font color=\"red\">3,3</font>   | 0.001          | Adam       | 0.3                | 3                | yes            |\n","| ...               | ...                 | 5,5          | 1,1      | 2,2        | <font color=\"red\">0.01</font>       | Adam       | 0.3                | 3                | yes            |\n","| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          |<font color=\"red\">SGD</font>     | 0.3                | 3                | yes            |\n","| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | <font color=\"red\">0.1</font>            | 3                | yes            |\n","| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | <font color=\"red\">2</font>            | yes            |\n","| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | 3                | <font color=\"red\">no</font>         |"]},{"cell_type":"markdown","metadata":{"id":"wAKv4vTBDziK"},"source":["---"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"W10 - CNN.ipynb","provenance":[],"collapsed_sections":["tsGwB9yQDziG","jmfUhECBDziI","BCdFsk9PDziK"]}},"nbformat":4,"nbformat_minor":0}